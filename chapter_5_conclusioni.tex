\chapter{Conclusioni}

Giunti al termine del progetto, la percezione è di aver appena sfiorato la superficie. Gli argomenti trattati sono stati numerosi, ma sono molteplici gli ambiti in cui un analisi più approfondita permetterebbe di raggiungere una maggiore consapevolezza sul ruolo dei \textit{memory allocators} nei sistemi operativi moderni. È stata costruita una base teorica solida per la nostra analisi e stabiliti alcuni punti chiave da considerare quando ci si interroga sulle metriche relative all'utilizzo della RAM.

Speriamo di essere stati in grado di sottolineare, durante il progetto, l'importanza del dedicare tempo e risorse alla scelta dell'algoritmo di assegnazione di memoria più appropriato per le proprie necessità. \textbf{Soprattutto durante lo sviluppo dei sistemi operativi, fare scelte oculate per quanto riguarda la gestione delle risorse può avere effetti che riverberano per l'efficienza di tutto il calcolatore.} È stato fatto un tentativo di dimostrare che l’allocazione non deve essere considerata in tutti i casi imprevedibile e per questo inaffidabile: dichiarare staticamente la memoria non è l'unica strada percorribile quando è richiesto ottenere determinati risultati. In contesti \textit{bare metal} e quindi in assenza di un sistema operativo, dove l'utente deve autonomamente amministrare le risorse fisiche, implementare un modulo di assegnazione adatto ai propri scopi è sicuramente un'alternativa valida e non da trascurarsi.

Con la nostra breve indagine sulla letteratura sull’argomento, le risorse che ci sono risultate utili per delineare un percorso didattico sono state menzionate: siamo consapevoli tuttavia che esistano numerose fonti che non abbiamo avuto modo e tempo di vagliare approfonditamente. Abbiamo descritto la specifica implementazione da noi sviluppata, evidenziando i pro e contro delle scelte implementative e le motivazioni alla base dell’adozione di determinati paradigmi. 

Sono state verificate in modo pratico le assunzioni teoriche suggerite dalla nostra indagine preliminare, e abbiamo delineato grossolanamente alcune strutture atte a descrivere i \textit{benchmark} a cui sottoporre i propri applicativi. Dedicare tempo all'analisi delle prestazioni del proprio programma utilizzando strumenti di varia natura può rivelare possibili congestioni e strade per rendere i processi più efficienti: chiaramente, poiché le riflessioni non sono volte specificatamente a un singolo campo di applicazione, non abbiamo potuto trarre conclusioni approfondite. Tuttavia, è sicuramente possibile basandosi sui dati emersi descrivere un quadro complesso e confermare l'importanza di continuare questa analisi con ulteriori accertamenti.

\section{Aree per ulteriore approfondimento}

I risultati ottenuti aprono la strada a ulteriori esperimenti e ottimizzazioni, come l’integrazione di politiche adattive o l'introduzione di sistemi che garantiscano la \textit{thread safety}. L'aver stabilito l'interfaccia \texttt{Allocator} permetterebbe con facilità di estendere gli strumenti da noi creati per supportare una grande varietà di \textit{memory allocators} che adottino politiche e meccanismi diversi, per indagare gli effetti delle differenze nell'approccio. Allo stesso modo, sicuramente meriterebbe attenzione la semplificazione e lo \textit{streamlining} delle infrastrutture per il \textit{logging}. In questo modo potremmo essere più vicini ad ottenere risultati che non ci permettano unicamente di fare un'analisi comparata ma anche di valutare le capacità dal punto di vista pratico in termini assoluti, escludendo l'\textit{overhead} eccessivo che osserviamo in questo momento.

\paragraph{Frammentazione esterna.}
Poiché le tipologie di allocatori da noi analizzate nel corso di questo progetto non presentano spesso criticità dovute alla \textit{external fragmentation}, non è stato ritenuto rilevante approfondire questa tematica quanto forse avrebbe meritato nel caso in cui le nostre riflessioni avessero anche riguardato ulteriori meccanismi di allocazione. Sebbene quindi al momento questa informazione sia mancante, utilizzando le infrastrutture già presenti non rappresenterebbe una problematica insormontabile integrarle.

\paragraph{Deferred coalescing.}
Un'area promettente per ottimizzazioni future riguarda lo studio dell'effetto di tecniche di \textit{deferred coalescing}, una pratica che consiste nel posticipare l'unione di blocchi liberi adiacenti a momenti strategici invece di essere eseguita immediatamente dopo ogni operazione di rilascio. Questo approccio potrebbe ridurre l'\textit{overhead} nelle operazioni di deallocazione frequenti, specialmente in scenari dove la frammentazione temporanea è accettabile. La sfida principale consiste nel determinare il momento ottimale per attivare il \textit{coalescing} e nel bilanciare il \textit{trade-off} tra memoria immediatamente disponibile e frammentazione accumulata.

\paragraph{Analisi della località dei dati.}
Analizzare la località spaziale permetterebbe di comprendere come la disposizione dei blocchi di memoria influisca sull'efficienza della cache, mentre uno studio della località temporale aiuterebbe a valutare la frequenza di riutilizzo dei blocchi e la durata delle allocazioni. Questi dati potrebbero essere utilizzati per ottimizzare le strategie di allocazione, ad esempio raggruppando oggetti con pattern di accesso simili o adottando politiche di placement che minimizzino i cache miss. Un'analisi approfondita della località, supportata da strumenti di profiling, potrebbe quindi fornire indicazioni preziose per migliorare ulteriormente le prestazioni degli allocatori in scenari reali.

\paragraph{Raccolta di traces reali.}
Per validare ulteriormente i risultati ottenuti, sarebbe estremamente utile raccogliere e analizzare trace reali provenienti da applicazioni di produzione in diversi domini (sistemi embedded, server ad alte prestazioni, applicazioni desktop). Questi dataset consentirebbero di testare gli allocatori in condizioni realistiche e variegate, identificando punti di forza e debolezza in scenari complessi che difficilmente possono essere riprodotti con benchmark sintetici. La creazione di una suite standardizzata di trace rappresentative potrebbe inoltre diventare un prezioso strumento per la comunità, facilitando confronti oggettivi tra diverse implementazioni di allocatori. 

\paragraph{Considerazioni sulla thread safety.}
Se si richiedesse di integrare l'implementazione degli allocatori in modo che essi siano \textit{thread safe}, i fattori da considerare aumenterebbero ulteriormente. La gestione concorrente delle liste concatenate nel \texttt{BuddyAllocator} richiederebbe l'uso di meccanismi di sincronizzazione più complessi (ad esempio, mutex o \textit{lock} per ogni lista), aumentando il rischio di contese e rallentando le operazioni in presenza di molti thread. Al contrario, la struttura compatta e lineare della \textit{bitmap} nel \texttt{BitmapBuddyAllocator} si presta meglio a implementazioni concorrenti, poiché è più semplice proteggere sezioni critiche e, in alcuni casi, è possibile sfruttare operazioni atomiche su bit per ridurre l'\textit{overhead} della sincronizzazione. Pertanto, in scenari \textit{multithreaded}, il \texttt{BitmapBuddyAllocator} risulta generalmente preferibile per la maggiore scalabilità e semplicità nella gestione della concorrenza.

\section{Risultati sugli allocatori analizzati}

Al termine di questa relazione, ripetiamo una parte dei riscontri avuti in base alla combinazione delle nostre osservazioni teoriche e esperienze pratiche sui meccanismi di gestione dinamica della memoria esplorati. L’analisi condotta sui diversi allocatori implementati — i.e. \texttt{SlabAllocator}, \texttt{BuddyAllocator} e \texttt{BitmapBuddyAllocator} — ha permesso di evidenziare punti di forza, limiti e scenari d’uso ottimali per ciascuna soluzione. 

\paragraph{SlabAllocator.}
Lo \texttt{SlabAllocator} si è dimostrato estremamente efficiente in tutti i casi in cui le richieste di memoria sono omogenee e di dimensione fissa. Esso è stato adoperato internamente nel nostro processo di sviluppo in maniera non dissimile da quanto avverrebbe all'interno di un sistema operativo: ciò fornisce prove concrete che specializzando i meccanismi per un sottoinsieme dei casi d'uso è possibile renderli molto più efficienti. Le operazioni di allocazione e rilascio avvengono in tempo costante, grazie alle strutture dati adoperate per organizzare i blocchi liberi. La semplicità della struttura riduce la probabilità di errori e rende il comportamento facilmente prevedibile. Tuttavia, la scelta della dimensione dello \textit{slab} in fase di inizializzazione è cruciale per evitare inefficienze dovute a una stima errata delle necessità applicative. 

\paragraph{BuddyAllocator.}
Il \texttt{BuddyAllocator} offre una buona soluzione per la gestione di richieste di memoria a taglia variabile, mantenendo bassa la frammentazione esterna grazie alla suddivisione e ricombinazione ricorsiva dei blocchi. I test hanno mostrato che, con una corretta parametrizzazione, l’allocatore è in grado di gestire efficacemente pattern di allocazione complessi (rampe, picchi, plateau), garantendo tempi di risposta rapidi soprattutto quando il numero di livelli non è eccessivo. Tuttavia, la struttura a potenze di due introduce una frammentazione interna potenzialmente elevata e l’implementazione tramite liste concatenate per ogni livello, sebbene semplice, può penalizzare la località della \textit{cache} e la scalabilità in scenari con molte allocazioni e deallocazioni frequenti. Ottimizzazioni come l’introduzione di flag per evitare la scansione lineare nel prevenire il \textit{double free} hanno migliorato le prestazioni, ma al costo di una minore robustezza rispetto a controlli più rigorosi.

\paragraph{BitmapBuddyAllocator.}
Il \texttt{BitmapBuddyAllocator} combina la flessibilità del \textit{buddy system} con la compattezza e l’efficienza delle \textit{bitmap}. I benchmark hanno evidenziato una migliore \textit{spatial cache locality} e una ridotta occupazione di memoria per i metadati rispetto al \texttt{BuddyAllocator} tradizionale. La gestione tramite \textit{bitmap} riduce l’\textit{overhead} delle operazioni di sincronizzazione e permette più facilmente il \textit{multithreading} attraverso l'uso di istruzioni atomiche per la manipolazione dei bit. Tuttavia, la complessità delle operazioni di scansione e aggiornamento della \textit{bitmap} può aumentare il numero di istruzioni eseguite, rallentando le prestazioni in alcuni scenari rispetto all’approccio con liste concatenate: ipotizziamo che diversi metodi potrebbero, introducendo forse un maggiore costo di spazio per la struttura, ridurre il costo computazione di queste operazioni.
Questa variante del \texttt{BuddyAllocator} è indicata in sistemi con memoria limitata per via della minore occupazione della stessa per i metadati. Anche in questo caso però, la frammentazione interna rimane un limite intrinseco della politica a potenze di due.

\paragraph{Considerazioni finali.}
Dai test emerge che la scelta dell’allocatore più adatto dipende fortemente dal pattern di utilizzo della memoria e dai vincoli del sistema. In ambienti dove la sua applicazione è appropriata, lo \texttt{SlabAllocator} è sempre preferibile. In contesti più generali, il \texttt{BuddyAllocator} offre un buon compromesso tra efficienza e flessibilità, mentre il \texttt{BitmapBuddyAllocator} si distingue per la migliore località della \textit{cache} e la semplicità di gestione in scenari concorrenti.

L’analisi dei \textit{benchmark} ha inoltre confermato l’importanza di valutare attentamente i parametri di inizializzazione (dimensione degli \textit{slab}, profondità dei livelli, dimensione minima dei blocchi) e di utilizzare strumenti di \textit{profiling} per tenere traccia dello stato della memoria e delle metriche in determinati momenti critici. In generale, nessun allocatore risulta universalmente superiore: la scelta deve essere guidata dalle caratteristiche specifiche dell’applicazione.


