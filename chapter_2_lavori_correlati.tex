\chapter{Riferimenti, influenze e fonti}

In questo capitolo vengono presentate le principali opere che hanno guidato lo sviluppo del progetto. L’allocazione dinamica della memoria rappresenta un tema centrale nell’ambito della programmazione dei sistemi di calcolo e, nel corso degli anni, è stata oggetto di numerosi studi e approfondimenti. La letteratura scientifica e didattica offre una panoramica ampia e articolata delle soluzioni proposte, delle problematiche affrontate e delle strategie adottate per gestire in modo efficiente le risorse di memoria, mentre le soluzioni pratiche che menzioniamo rappresentano tentativi concreti di affrontare la tematica. 

L’obiettivo di questa sezione è fornire una sintesi ragionata dei contributi che sono a noi sembrati più rilevanti, evidenziando gli aspetti, tanto astratti quanto pragmatici, che hanno influenzato le scelte progettuali. Le fonti relative alle politiche di allocazione oggetto d'esame sono esplorate più a fondo anche nel capitolo successivo per contestualizzarle rispetto all'implementazione. Verranno inoltre illustrate le principali fonti di ispirazione per la struttura del progetto.

\section{Letteratura scientifica sull’allocazione dinamica}

L’articolo \textit{``Dynamic Storage Allocation, A Survey and Critical Review''} di P. Wilson et al.\cite{wilson1995} è stato adottato come riferimento storico: in particolare il capitolo 4 presenta un sunto della letteratura pubblicata sull’argomento e delle soluzioni proposte negli trenta anni precedenti alla pubblicazione. L'indagine risale al 1995 ed è dunque datata, tuttavia è stata scelta in quanto moltissime risorse sull'argomento ne fanno menzione positiva: nostra convinzione è che il ruolo dello studio di Wilson non possa essere trascurato, in particolare nello aver spronato numerosi altri approfondimenti in materia. L'unico argomento che l'articolo non tocca sembra essere il \textit{multithreading}, che sarà una nota a margine anche nel nostro prog. 

Gli autori sottolineano argutamente che l'allocazione dinamica è un quesito ``per lo più considerato essere già risolto o irrisolvibile''. Le conseguenze di questa incomprensione sono immediate: l’allocazione dinamica è considerata un problema ``risolto'' per chi abbia abbondanti risorse computazionali a disposizione e contemporaneamente ``irrisolvibile'' in contesti dove vi siano importanti limitazioni temporali o spaziali. 

Un punto critico che emerge in più punti della letteratura riguarda le differenze tra i \textit{benchmark} sintetici spesso usati per valutare gli allocatori e i carichi di lavoro che si possono incontrare nel mondo reale. Queste sequenze di operazioni raramente riflettono le profonde correlazioni e le sistematiche interazioni tra allocazioni e deallocazioni. La mancata comprensione di questi collegamenti causa interpretazioni errate dei risultati, che sono dunque inadatti a rappresentare l’efficienza effettiva degli allocatori. A tal proposito, lo studio \textit{``Real-Time Performance of Dynamic Memory Allocation Algorithms''} di I. Puaut\cite{puaut2002} offre un contributo prezioso, svolgendo il pregevole lavoro di combinare test (reali e sintetici) con precisi studi analitici. Nessuna possibilità è lasciata inesplorata ed è dimostrato che, in determinate condizioni, è possibile predire con un certo grado di precisione il comportamento degli allocatori di memoria in casi dove è essenziale che essi rispettino determinati parametri per giustificarne l’applicazione.

Le conclusioni delle esperienze di Puaut confermano le tesi di Wilson: \textbf{l’inefficienza non risiede negli allocatori stessi, quanto nella mancata comprensione del loro funzionamento}. Il timore nella percepita inefficacia della gestione dinamica conduce a scelte inappropriate. Essa presenta certamente diverse sfide, ma attraverso caute valutazioni è possibile applicarla anche laddove tradizionalmente è stata preferita l’allocazione statica.

Gli autori del \textit{survey} continuano, sottolineando che soluzioni efficienti per la gestione dinamica di memoria fanno uso di ``regolarità'' nel comportamento del programma. Infatti, osservando come si avvicendano le operazioni dell'allocatore è possibile scegliere la corretta politica di gestione per il proprio caso d’uso. Non esiste dunque una risposta \textit{``set and forget''} e invece risulta essere saggio dedicare tempo e lavoro all’esplorazione di diverse soluzioni. Nei capitoli successivi l’articolo definisce una chiara tassonomia delle principali specie di allocatori, la quale avremo modo di approfondire nel capitolo terzo.

\section{Didattica degli allocatori}

Poiché la memoria dinamicamente allocata è un aspetto cardine del linguaggio C e dei sistemi operativi (e di tutta la programmazione a basso livello), la letteratura didattica a riguardo è ampia. Di nota per la comprensione del funzionamento e del ruolo dei gestori dinamici sono i libri \textit{``The C Programming Language''} (capitolo 8.7, \textit{``Example – A Storage Allocator''}) di B. Kernighan e D. Ritchie\cite{kernighan1988} e \textit{``Computer Systems – A Programmer’s Perspective''} (capitolo 9.9, \textit{``Dynamic Memory Allocation''}) di R. Bryant\cite{bryant2015}. Illuminante è stato il capitolo \textit{``Dynamic Storage Allocation''} del volume primo di \textit{``The Art of Computer Programming''}, di D. Knuth\cite{knuth1997}. In quest'ultimo volume l'autore svolge una dettagliata analisi degli algoritmi comunemente adottati nel progetto degli allocatori di memoria, discutendo l’analisi matematica su cui essi si basano e fornendo chiari esempi e illustrazioni.

Nel libro di Kernighan e Ritchie abbiamo un esempio pratico di implementazione di un allocatore lineare a blocchi di dimensione variabili, attraverso l’uso di una lista concatenata per mantenere un indice dei blocchi liberi e che, in risposta a una operazione di \textit{free}, unisce blocchi adiacenti. Questa implementazione è descritta dagli stessi autori come ``semplice e immediata'' e funge da dimostrazione del fatto che ``sebbene l’allocazione dello \textit{storage} sia intrinsecamente dipendente dall’architettura fisica, le dipendenze dalla macchina possono essere controllate e confinate a una parte molto piccola del programma.''

Il secondo volume citato, ad opera di Bryant, definisce a nostro avviso la principale fonte di frustrazione in modo cristallino: secondo l’autore, ``i programmatori ingenui spesso presumono erroneamente che la memoria virtuale sia una risorsa illimitata. In realtà, la quantità totale di memoria virtuale allocata da tutti i processi di un sistema è limitata dalla quantità di spazio di \textit{swap} su disco. I bravi programmatori sanno che la memoria virtuale è una risorsa finita che deve essere utilizzata in modo efficiente.'' Questa osservazione è più che mai rilevante quando o dispositivi di archiviazione permanente menzionati non esistono e laddove il costo di accedere al disco sia particolarmente oneroso e per questo ingiustificabile, come nella progettazione dei sistemi operativi.

\textbf{La reale criticità non consiste in un debito tecnologico, in limiti intrinseci o in euristiche inefficienti, bensì in cattive abitudini dei programmatori.} Il risultato di questa percezione è apparente nell’assenza del riconoscimento dell’importanza degli allocatori quando la loro efficienza non sia strettamente indispensabile. Nei contesti in cui invece essa lo sia, viene spesso scelto di adoperare artefici di gestione della memoria che evitano la componente dinamica, sacrificando spazio e prestazioni in cambio di una complessità sibillina e artificiosa, che li rende di difficile manutenzione e applicabilità al di fuori del contesto per cui sono stati concepiti. Riprendendo brevemente Wilson,

\begin{quote}
``The impact on software clarity, flexibility, maintainability, and reliability is significant, though difficult to estimate. It should not be underestimated, however, because these hidden costs can incur major penalties in productivity--- and, to put it plainly, human costs in sheer frustration, anxiety, and general suffering.'
\end{quote}
\begin{flushright}
(Wilson, \textit{Dynamic Storage Allocation, A Survey and Critical Review}~\cite{wilson1995}, ch. 1.1)
\end{flushright}

L’autore continua definendo i quattro problemi che ogni implementazione di un \textit{memory allocator} deve risolvere. Sottolineiamo che queste necessità si manifestano nel caso in cui si decida che l’allocatore debba essere \textit{general use}: in casi particolari, si può decidere di sacrificare la generalità dell’allocatore in cambio di migliori risultati\footnotemark. Le attività da intraprendere sono:
\begin{enumerate}
  \item \textbf{L’organizzazione dei blocchi liberi} in memoria;
  \item La scelta del blocco \textbf{corretto} a seguito di una richiesta;
  \item \textbf{Il meccanismo di \textit{splitting}} in blocchi di memoria delle dimensioni necessarie;
  \item \textbf{Le modalità di \textit{coalescing}} di blocchi liberi per poter soddisfare richieste future.
\end{enumerate}
Nel corso della relazione, descriveremo volta per volta come queste necessità siano affrontate in tutte le specifiche implementazioni, sottolineando il costo della nostra soluzione, così come i compromessi accettati.

\footnotetext{Vedremo successivamente nello \texttt{SlabAllocator} un tale caso.}

Di particolare importanza è stata l’analisi di \textit{dlmalloc}\cite{dlmalloc}, l’allocatore di memoria sviluppato da Doug Lea intorno agli anni novanta del secolo scorso. Esso ha fornito le basi per \textit{ptmalloc}, una \textit{fork} modificata per essere \textit{thread safe} da Wolfram Gloger e che successivamente è stata adottata dalla \textit{glibc} (\textit{GNU C library}). Studiare questa implementazione è stato particolarmente utile in quanto rappresenta un esempio di allocatore dinamico con \textit{chunk} di dimensioni variabili largamente adoperato e documentato. Inoltre, è stato interessante studiare un accenno su come il problema dell’accesso concorrente sia stato risolto attraverso \textit{mutex} e ``arene''.

\section{Ispirazione per la struttura}
Il progetto si basa principalmente sull’implementazione dello \texttt{SlabAllocator} e \texttt{BuddyAllocator} viste durante le lezioni del corso di Sistemi Operativi tenuto dal professor Grisetti. Tuttavia, la struttura presenta sostanziali differenze, che rendono le procedure diverse: in primo luogo, la decisione di adottare il paradigma dell'\textit{Object Oriented Programming} usando il linguaggio C, un approccio che è sembrato adatto ai nostri fini promosso durante il corso. Le altre scelte sono esplorate più nel dettaglio nel capitolo successivo.

\paragraph{Object Oriented C Programming: perché?}

Per approfondire il tema è stato consultato il libro \textit{Object-Oriented Programming With ANSI-C} del professor Axel-Tobias Schreiner\cite{schreiner1994}. Nonostante non sia stato ritenuto di applicarne interamente gli insegnamenti per semplicità, il testo si è rivelato essere un utile riferimento teorico. La decisione di usare C piuttosto che un linguaggio con supporto esplicito a questo paradigma, come i suoi discendenti C++ o C\#, nasce da un’esigenza didattica di ``squarciare il velo di Maya'' che spesso avvolge i meccanismi alla base della programmazione orientata agli oggetti.

\textbf{In particolare, l'obiettivo è sottolineare come la gestione dell’allocazione dinamica di memoria, strettamente legata all’architettura fisica del calcolatore, sia un aspetto fondamentale della programmazione a basso livello.} Scegliendo di modellare consapevolmente concetti che in altri linguaggi sono automaticamente gestiti dal compilatore si acquisisce maggiore consapevolezza sui dettagli implementativi e si sottolineano importanti punti per la comprensione di nozioni quali \textit{memory leak}, \textit{dangling pointers}, ciclo di vita, costruttori e distruttori. Colui che per la prima volta decida di approcciare il linguaggio C trova nel semplice uso di \texttt{malloc} e \texttt{free} le prime grandi ``responsabilità'' da programmatore: un’obbligazione a gestire autonomamente e responsabilmente una risorsa, che porta a un livello di consapevolezza maggiore sui meccanismi interni e le \textit{routine} che costituiscono i sistemi operativi. 

\paragraph{Risorse online e analisi comparative}

Sono stati di riferimento per lo sviluppo le pubblicazioni dell’utente \textit{mtrebi}\cite{mtrebi} e di Emery Berger, professore presso l’Università di Massachusetts Amherst\cite{emeryberger}, pubblicate su GitHub: il primo ha fornito chiare indicazioni sul funzionamento e i compromessi tra diverse tipologie di gestori di memoria, mentre il secondo ha offerto una preziosa analisi storica, catalogando diversi popolari algoritmi di allocazione che si sono succeduti nei sistemi operativi del passato, giungendo alle scoperte moderne. Ciò ha permesso di osservare l’evoluzione nel tempo delle soluzioni per l'assegnazione dinamica di memoria.

Ulteriori approfondimenti sono stati tratti da risorse didattiche, tra cui il tutorial su \texttt{malloc} della \textit{École Pour l'Informatique et les Techniques Avancées}\cite{epita-malloc} e i materiali del corso della Australian National University (COMP2310)\cite{anu-malloc}, incentrati sull’implementazione pratica. Il progetto del corso CS3410 della Cornell University\cite{cornell-malloc} ha fornito spunti utili per la gestione di \textit{edge case} e ottimizzazioni, mentre il saggio di Dmitry Soshnikov\cite{soshnikov-allocator} è risultato molto chiaro e informativo. Infine, le slide della Czech Technical University (CVUT)\cite{cvut-dynamic-mem} hanno offerto una panoramica sistematica sulle politiche di allocazione e sulle metriche di valutazione delle prestazioni.

Queste risorse, insieme a un’analisi comparativa degli approcci esistenti, hanno guidato la progettazione degli allocatori, con particolare attenzione ai \textit{trade-off} tra efficienza, frammentazione e scalabilità. Continuiamo approfondendo questi parametri e descrivendo la nostra implementazione.