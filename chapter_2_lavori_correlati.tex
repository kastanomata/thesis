\chapter{Lavori Correlati/Basi}
\section{Object Oriented C Programming: perché?}

Per approfondire il tema della programmazione \textit{OOP} in \textbf{C} è stato consultato il libro \textit{Object-Oriented Programming With ANSI-C} del professor Axel-Tobias Schreiner~\cite{schreiner1994}. Nonostante non sia stato ritenuto di applicarne interamente gli insegnamenti per semplicità, il testo si è rivelato essere un utile riferimento teorico. La decisione di usare \textbf{C} piuttosto che un linguaggio che fornisce supporto diretto a questo paradigma, come \textbf{C++} o \textbf{C\#}, nasce da un’esigenza didattica di ``squarciare il velo di Maya'' che spesso avvolge i meccanismi alla base della programmazione orientata agli oggetti.

In particolare, si è ritenuto di voler sottolineare come la gestione dell’allocazione dinamica di memoria, strettamente legata all’architettura fisica del calcolatore, sia un aspetto fondamentale della programmazione a basso livello. Colui che per la prima volta decida di approcciare il linguaggio \textbf{C} trova nel semplice uso di \textit{malloc} e \textit{free} le prime grandi ``responsabilità'' da programmatore: un’obbligazione a gestire autonomamente e responsabilmente una risorsa, che porta a un livello di consapevolezza maggiore sui meccanismi interni e le routine che costituiscono i sistemi operativi.

Scegliendo di modellare consapevolmente concetti che in \textbf{C++} sono automaticamente gestiti dal compilatore si acquisisce maggiore consapevolezza sui dettagli implementativi e si sottolineano importanti punti per la comprensione di nozioni quali \textit{memory leak}, \textit{dangling pointers}, ciclo di vita, costruttori e distruttori.

\section{Letteratura scientifica sull’allocazione dinamica}

L’articolo \textit{Dynamic Storage Allocation, A Survey and Critical Review} di P. Wilson et al.~\cite{wilson1995} è stato adottato come riferimento storico: in particolare il capitolo 4 presenta un sunto della letteratura pubblicata sull’argomento negli anni precedenti e delle soluzioni proposte per affrontare il problema, che gli autori sottolineano argutamente essere ``per lo più considerato essere già risolto o irrisolvibile''. Un punto critico che emerge infatti in più punti della letteratura riguarda le differenze tra i \textit{benchmark} sintetici usati per valutare gli allocatori e i carichi di lavoro reali. Le suite di test, infatti, raramente riflettono le profonde correlazioni e le sistematiche interazioni tra allocazioni e deallocazioni. La mancata comprensione di questi collegamenti causa incomprensioni e interpretazioni errate dei risultati di questi test, che sono dunque inadatti a rappresentare l’efficienza degli allocatori nel mondo reale.

Le conseguenze di questa divergenza sono immediate: l’allocazione dinamica è considerata un problema ``risolto'' per chi abbia abbondanti risorse computazionali a disposizione e contemporaneamente ``irrisolvibile'' in contesti dove vi siano importanti limitazioni temporali o spaziali. A tal proposito, lo studio \textit{Real-Time Performance of Dynamic Memory Allocation Algorithms} di I. Puaut~\cite{puaut2002} offre un contributo prezioso, svolgendo il pregevole lavoro di combinare test (reali e sintetici) con precisi studi analitici. Nessuna possibilità è lasciata inesplorata ed è dimostrato che, in determinate condizioni, è possibile realmente predire il comportamento degli allocatori di memoria in casi dove è essenziale che essi rispettino determinati parametri per giustificarne l’applicazione.

Le conclusioni delle esperienze di Puaut confermano le tesi di Wilson: l’inefficienza non risiede negli allocatori stessi, quanto nella mancata comprensione del loro funzionamento. Il timore nella percepita inefficienza dell’allocazione dinamica porta a scelte inappropriate. Essa presenta certamente diverse sfide, ma attraverso caute valutazioni è possibile applicarla anche laddove tradizionalmente viene preferita l’allocazione statica.

\begin{quote}
``Such problems may be hidden because most programmers who encounter severe issues may simply code around them using ad-hoc storage management techniques---or, as is still painfully common, by statically allocating ``enough'' memory for variable-sized structures. These ad-hoc approaches to memory management lead to `brittle' software with hidden limitations (e.g., due to the use of fixed-size arrays). The impact on software clarity, flexibility, maintainability, and reliability is significant, though difficult to estimate. It should not be underestimated, however, because these hidden costs can incur major penalties in productivity---and, to put it plainly, human costs in sheer frustration, anxiety, and general suffering.''
\end{quote}
\begin{flushright}
\textit{(Wilson~\cite{wilson1995}, capitolo 1.1)}
\end{flushright}

Gli autori del survey continuano, sottolineando che soluzioni efficienti per la gestione dinamica di memoria fanno uso di ``regolarità'' nel comportamento del programma. Infatti, osservando come viene allocata e deallocata la memoria è possibile scegliere la corretta politica di gestione per il proprio caso d’uso. Non esiste dunque una soluzione \textit{``set and forget''} e invece risulta essere appropriato dedicare risorse all’esplorazione di diverse soluzioni. Successivamente l’articolo definisce una chiara tassonomia delle principali specie di allocatori, la quale avremo modo di approfondire nel capitolo terzo.

\section{Didattica degli allocatori}

Poiché la memoria dinamicamente allocata è un aspetto cardine del linguaggio \textbf{C} e dei sistemi operativi (e di tutta la programmazione a basso livello), la letteratura didattica a riguardo è ampia. Di nota per la comprensione del funzionamento e del ruolo dei gestori dinamici della memoria sono i libri \textit{The C Programming Language} (capitolo 8.7, ``Example – A Storage Allocator'') di B. Kernighan e D. Ritchie~\cite{kernighan1988} e \textit{Computer Systems – A Programmer’s Perspective} (capitolo 9.9, ``Dynamic Memory Allocation'') di R. Bryant~\cite{bryant2015}. Illuminante è stato il capitolo \textit{Dynamic Storage Allocation} del volume primo di \textit{The Art of Computer Programming}, di D. Knuth~\cite{knuth1997}. Quest’ultimo volume va nel dettaglio spiegando l’analisi matematica che supporta le euristiche comunemente adottate nel progetto degli allocatori di memoria, fornendo chiari esempi e illustrazioni.

Nel libro di Kernighan e Ritchie abbiamo un esempio pratico di implementazione di un allocatore lineare a blocchi di dimensione variabili, attraverso l’uso di una \textit{Linked List} per mantenere un indice dei blocchi liberi e che, in risposta a una operazione di \textit{free}, unisce blocchi adiacenti. Questa implementazione descritta dagli stessi autori come ``semplice e immediata'' funge da dimostrazione del fatto che ``sebbene l’allocazione dello storage sia intrinsecamente dipendente dall’architettura fisica, il codice illustra come le dipendenze dalla macchina possano essere controllate e confinate a una parte molto piccola del programma.''

Il secondo volume citato, ad opera di Bryant, definisce a nostro avviso in modo cristallino quale sia la principale fonte del problema. Secondo l’autore, ``I programmatori ingenui spesso presumono erroneamente che la memoria virtuale sia una risorsa illimitata. In realtà, la quantità totale di memoria virtuale allocata da tutti i processi di un sistema è limitata dalla quantità di spazio di swap su disco. I bravi programmatori sanno che la memoria virtuale è una risorsa finita che deve essere utilizzata in modo efficiente.'' Questa osservazione è più che mai rilevante in contesti come la programmazione \textit{embedded} e \textit{real time}, così come nella progettazione di sistemi operativi.

La reale criticità nel mondo dell’allocazione dinamica non consiste in un debito tecnologico, in limiti intrinseci o in euristiche inefficienti, bensì in cattive abitudini dei programmatori. Il risultato di questa percezione è apparente nell’assenza di riconoscimento dell’importanza degli allocatori quando la loro efficienza non sia strettamente indispensabile. Nei contesti in cui invece essa lo sia, viene spesso scelto di adoperare artefici di gestione della memoria che evitano la componente dinamica, sacrificando spazio e prestazioni in cambio di una complessità sibillina e artificiosa, che li rende di difficile manutenzione e applicabilità al di fuori del contesto per cui sono stati concepiti.

L’autore continua definendo i quattro problemi che ogni implementazione di un gestore dinamico di memoria deve risolvere. Sottolineiamo che queste necessità si manifestano nel caso in cui si decida che l’allocatore debba essere \textit{general use}, che sono l’oggetto di analisi in corso. In casi particolari, si può decidere di sacrificare la generalità dell’allocatore in cambio di risultati migliori. Essi sono:
\begin{enumerate}
  \item L’organizzazione dei blocchi liberi in memoria;
  \item La scelta del blocco corretto a seguito di una richiesta;
  \item Il meccanismo di \textit{splitting} in blocchi di memoria delle dimensioni necessarie;
  \item Le modalità di \textit{coalescing} di blocchi liberi per poter soddisfare richieste future.
\end{enumerate}
Nel corso delle descrizioni del nostro progetto, descriveremo come li abbiamo affrontati in tutte le specifiche implementazioni, sottolineando il costo della nostra soluzione, così come i compromessi accettati.

Di particolare importanza è stata l’analisi di \textit{dlmalloc}, l’allocatore di memoria sviluppato da Doug Lea intorno agli anni novanta del secolo scorso~\cite{dlmalloc}. Esso ha fornito le basi per \textit{ptmalloc}, una fork modificata per essere \textit{thread-safe} da Wolfram Gloger e che successivamente è stata adottata dalla \textit{glibc} (\textit{GNU C library}). Studiare questa implementazione è stato particolarmente utile in quanto rappresenta un esempio di allocatore dinamico di memoria con \textit{chunk} di dimensioni variabili largamente adoperato e documentato. Inoltre, è stato interessante studiare come il problema dell’accesso concorrente sia stato risolto attraverso \textit{mutex} e ``arene'', nonostante nella nostra implementazione non siano state integrate soluzioni per affrontare il problema del \textit{multithreading}.

\section{Ispirazione per la struttura}
Il progetto si basa principalmente sull’implementazione dello \textit{SlabAllocator} e \textit{BuddyAllocator} vista durante le lezioni del corso di Sistemi Operativi tenuto dal professor Grisetti. Tuttavia, la struttura presenta sostanziali differenze, che rendono le procedure leggermente diverse. Sono esplorate più nel dettaglio nel capitolo successivo.

Sono stati di riferimento per lo sviluppo le pubblicazioni dell’utente \textbf{mtrebi}~\cite{mtrebi} e di Emery Berger, professore presso l’Università di Massachusetts Amherst~\cite{emeryberger} su Github: il primo ha fornito chiare indicazioni sul funzionamento e i compromessi tra diverse tipologie di allocatori di memoria, mentre il secondo ha offerto una preziosa analisi storica, catalogando diversi popolari algoritmi di allocazione che si sono succeduti nel corso del tempo. Ciò ha permesso di osservare l’evoluzione nel tempo delle soluzioni per l’allocazione dinamica di memoria.